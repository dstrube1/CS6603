{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "19edc3ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete\n"
     ]
    }
   ],
   "source": [
    "# If running for the first time:\n",
    "# %pip install gensim\n",
    "# %pip install opencv-python\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "import gensim.models\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "\n",
    "newmodel = gensim.models.KeyedVectors.load_word2vec_format('reducedvector.bin', binary=True)\n",
    "\"\"\"\n",
    "# Find the five nearest neighbors to the word man \n",
    "print('Five nearest neighbors to the word man: ')\n",
    "print(newmodel.most_similar('man', topn=5))\n",
    "# [('woman', 0.5876938104629517), ('girl', 0.5229198932647705), ('young', 0.49715912342071533), ('immortal', 0.4890766441822052), ('spider', 0.47930291295051575)]\n",
    "\n",
    "# Compute a measure of similarity between woman and man \n",
    "print('A measure of similarity between woman and man: ')\n",
    "print(newmodel.similarity('woman', 'man'))\n",
    "# 0.5876938\n",
    "\n",
    "# To complete analogies like man is to woman as king is to ??, we can use:\n",
    "print('Man is to woman as king is to ??:')\n",
    "print(newmodel.most_similar(positive=['king', 'woman'], negative=['man'], topn=1))\n",
    "# [('queen', 0.5532454252243042)]\n",
    "\n",
    "\"\"\"\n",
    "print('Setup complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4352d649",
   "metadata": {},
   "source": [
    "Task Set #1\n",
    "\n",
    "Q1: We will use the target words - man and woman. Use the pre-trained word2vec model to rank the following 15 words from the most similar to the least similar to each target word. For each word-target word pair, provide the similarity score. Provide your results in table format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4cd3f17b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorted man list:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>man</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>woman</td>\n",
       "      <td>0.587694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>child</td>\n",
       "      <td>0.333422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>doctor</td>\n",
       "      <td>0.289247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>wife</td>\n",
       "      <td>0.283479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>king</td>\n",
       "      <td>0.264497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>husband</td>\n",
       "      <td>0.234116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>nurse</td>\n",
       "      <td>0.153481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>birth</td>\n",
       "      <td>0.123439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>scientist</td>\n",
       "      <td>0.112269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>queen</td>\n",
       "      <td>0.110419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>professor</td>\n",
       "      <td>0.107622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>teacher</td>\n",
       "      <td>0.098740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>president</td>\n",
       "      <td>0.094579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>engineer</td>\n",
       "      <td>0.087364</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         word     score\n",
       "0         man  1.000000\n",
       "1       woman  0.587694\n",
       "2       child  0.333422\n",
       "3      doctor  0.289247\n",
       "4        wife  0.283479\n",
       "5        king  0.264497\n",
       "6     husband  0.234116\n",
       "7       nurse  0.153481\n",
       "8       birth  0.123439\n",
       "9   scientist  0.112269\n",
       "10      queen  0.110419\n",
       "11  professor  0.107622\n",
       "12    teacher  0.098740\n",
       "13  president  0.094579\n",
       "14   engineer  0.087364"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sorted woman list:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>woman</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>child</td>\n",
       "      <td>0.589809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>man</td>\n",
       "      <td>0.587694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>husband</td>\n",
       "      <td>0.449643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>birth</td>\n",
       "      <td>0.420309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>wife</td>\n",
       "      <td>0.300689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>nurse</td>\n",
       "      <td>0.254358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>queen</td>\n",
       "      <td>0.228572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>teacher</td>\n",
       "      <td>0.204078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>doctor</td>\n",
       "      <td>0.196134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>scientist</td>\n",
       "      <td>0.137311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>king</td>\n",
       "      <td>0.122529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>professor</td>\n",
       "      <td>0.105199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>president</td>\n",
       "      <td>0.084627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>engineer</td>\n",
       "      <td>0.044264</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         word     score\n",
       "0       woman  1.000000\n",
       "1       child  0.589809\n",
       "2         man  0.587694\n",
       "3     husband  0.449643\n",
       "4       birth  0.420309\n",
       "5        wife  0.300689\n",
       "6       nurse  0.254358\n",
       "7       queen  0.228572\n",
       "8     teacher  0.204078\n",
       "9      doctor  0.196134\n",
       "10  scientist  0.137311\n",
       "11       king  0.122529\n",
       "12  professor  0.105199\n",
       "13  president  0.084627\n",
       "14   engineer  0.044264"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "input_list=['wife', 'husband', 'child', 'queen', 'king', 'man', 'woman', 'birth', 'doctor', 'nurse', 'teacher', \n",
    "            'professor', 'engineer', 'scientist', 'president']\n",
    "man_dict = {}\n",
    "woman_dict = {}\n",
    "for item in input_list:\n",
    "    man_value = newmodel.similarity('man', item)\n",
    "    woman_value = newmodel.similarity('woman', item)\n",
    "    man_dict[item] = man_value\n",
    "    woman_dict[item] = woman_value\n",
    "\n",
    "    \n",
    "def print_table_from_dict(dictionary, name):\n",
    "    # https://www.tutorialsteacher.com/articles/sort-dict-by-value-in-python\n",
    "    ionary = sorted(dictionary.items(), key=lambda x:x[1], reverse=True)\n",
    "    # ^^^ was going to call this d_list, but then I thought of this funny thing:\n",
    "    dictionary = dict(ionary)\n",
    "\n",
    "    # doesn't really look like a table using either of these:\n",
    "    # print(dictionary)\n",
    "    # display(dictionary)\n",
    "\n",
    "    # Must re-index to put into a dataframe:\n",
    "    dictionary = {'word':dictionary.keys(), 'score':dictionary.values()}\n",
    "    # https://www.geeksforgeeks.org/how-to-convert-dictionary-to-pandas-dataframe/\n",
    "    data = pd.DataFrame.from_dict(dictionary)\n",
    "    print(f\"Sorted {name} list:\")\n",
    "    display(data)\n",
    "    print()\n",
    "\n",
    "print_table_from_dict(man_dict, 'man')\n",
    "print_table_from_dict(woman_dict, 'woman')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32edd5e1",
   "metadata": {},
   "source": [
    "Q2: The Bigger Analogy Test Set (BATS) Word analogy task has been one of the standard benchmarks for word embeddings since 2013 (https://vecto.space/projects/BATS/ ). \n",
    "\n",
    "A) Select any file from the downloaded dataset (BATS_3.0.zip). For each row in your selected file, choose a target word from the row and provide the measure of similarity between your target word and the other words on the row (Remember to document the file used). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "902cbdf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected file: BATS_3.0/3_Encyclopedic_semantics/E09 [things - color].txt:\n",
      "\n",
      "               0          1         2\n",
      "0            ant      black  0.016854\n",
      "1            ant      brown  0.164345\n",
      "2            ant        red  0.086045\n",
      "3          apple        red  0.169385\n",
      "4          apple     orange  0.204806\n",
      "5          apple     yellow  0.117173\n",
      "6          apple     golden  0.149683\n",
      "7     blackboard      black  0.002111\n",
      "8     blackboard      green  0.038799\n",
      "9          blood        red  0.239439\n",
      "10     blueberry       blue  0.145673\n",
      "11     blueberry      black  0.102351\n",
      "12      broccoli      green  0.220847\n",
      "13        bruise       blue  0.250219\n",
      "14        bruise     purple  0.402197\n",
      "15       cabbage      green   0.21119\n",
      "16        carrot     orange  0.199563\n",
      "17        carrot        red  0.135183\n",
      "18        carrot     yellow  0.237842\n",
      "19   cauliflower      white  0.178381\n",
      "20   cauliflower      green  0.287544\n",
      "21   cauliflower     yellow  0.309281\n",
      "22   cauliflower  yellowish  0.464636\n",
      "23        celery      green  0.272918\n",
      "24        celery      white  0.198108\n",
      "25        celery      brown  0.281928\n",
      "26        cherry        red  0.260896\n",
      "27        cherry     yellow  0.313398\n",
      "28        cherry      black  0.221335\n",
      "29     chocolate      white   0.30966\n",
      "30     chocolate      brown  0.313166\n",
      "31     chocolate      black  0.276576\n",
      "32         cloud      white  0.206877\n",
      "33         cloud       gray  0.208631\n",
      "34         cloud       grey  0.144232\n",
      "35          coal      black  0.072756\n",
      "36        coffee      black  0.186443\n",
      "37        coffee      brown  0.204282\n",
      "38     cranberry        red  0.125799\n",
      "39     cranberry     purple  0.273153\n",
      "40     cranberry       pink  0.297515\n",
      "41         cream      white  0.334988\n",
      "42          crow      black  0.231378\n",
      "43      cucumber      green  0.231164\n",
      "44       emerald      green  0.430846\n",
      "45          frog      green  0.368224\n",
      "46          frog      brown  0.401646\n",
      "47          frog       grey  0.373885\n",
      "48          frog       gray  0.420906\n",
      "49        grapes      black   0.22214\n",
      "50        grapes        red  0.193513\n",
      "51        grapes      green  0.234692\n",
      "52        grapes     purple  0.363582\n",
      "53         grass      green  0.393804\n",
      "54        leaves      green   0.38611\n",
      "55        leaves        red  0.239475\n",
      "56        leaves     yellow    0.3729\n",
      "57          milk      white  0.226181\n",
      "58         paper      white  0.243027\n",
      "59         paper      color  0.299122\n",
      "60       parsley      green  0.221835\n",
      "61        pepper      black  0.305234\n",
      "62        pepper        red  0.156652\n",
      "63        pepper      green   0.26711\n",
      "64        pepper     yellow  0.261175\n",
      "65        pepper     orange   0.30587\n",
      "66        potato      brown  0.205794\n",
      "67        radish        red  0.188842\n",
      "68        radish       pink  0.379294\n",
      "69        radish      white   0.23589\n",
      "70        radish      green  0.260944\n",
      "71        radish      black  0.217528\n",
      "72         raven      black  0.152673\n",
      "73          rose        red  0.129654\n",
      "74          rose     yellow  0.123819\n",
      "75          rose       pink  0.183255\n",
      "76          rose      white  0.201757\n",
      "77          rose       blue  0.162757\n",
      "78          ruby        red  0.126898\n",
      "79          salt      white  0.178166\n",
      "80      sapphire       blue  0.357904\n",
      "81           sea       blue  0.195446\n",
      "82           sea      green  0.182578\n",
      "83           sea       gray  0.065837\n",
      "84           sea       grey  0.124886\n",
      "85           sky       blue   0.44397\n",
      "86           sky       gray  0.167499\n",
      "87           sky       grey  0.217461\n",
      "88          snow      white  0.385684\n",
      "89          soil      black  0.169323\n",
      "90          soil      brown  0.127698\n",
      "91          soil       dark  0.137971\n",
      "92       spinach      green  0.170907\n",
      "93         sugar      white  0.190748\n",
      "94         sugar      brown  0.170382\n",
      "95           sun     yellow  0.152244\n",
      "96           sun       gold  0.029353\n",
      "97          swan      white  0.395133\n",
      "98          swan      black   0.33638\n",
      "99          swan       gray  0.461392\n",
      "100         swan       grey  0.425437\n",
      "101          tea      black  0.245288\n",
      "102          tea      green  0.402273\n",
      "103          tea      white  0.283473\n",
      "104          tea        red  0.316095\n",
      "105          tea      brown   0.22695\n",
      "106          tea     yellow  0.344161\n",
      "107       tomato        red  0.139281\n",
      "108   toothpaste      white  0.096497\n",
      "109      yoghurt      white  0.092022\n",
      "110      yoghurt       pink  0.254411\n"
     ]
    }
   ],
   "source": [
    "file_path = 'BATS_3.0/3_Encyclopedic_semantics/E09 [things - color].txt'\n",
    "data = pd.read_csv(file_path, sep=\"\\s+\", header=None).rename({0: \"target\", 1:\"words\"}, axis=1)\n",
    "\n",
    "data_list = [(data.at[index, \"target\"], data.at[index, \"words\"].split(\"/\")) for index in range(data.shape[0])]\n",
    "\n",
    "scores = {}\n",
    "index = 0\n",
    "for target, words in data_list:\n",
    "    for word in words:\n",
    "        try:\n",
    "            score = (target, word, newmodel.similarity(target, word))\n",
    "            scores[index] = score\n",
    "            index += 1\n",
    "        except KeyError:\n",
    "            # print(f\"{word} not found.\")\n",
    "            continue\n",
    "\n",
    "print(f\"Selected file: {file_path}:\\n\")\n",
    "\n",
    "data = pd.DataFrame.from_dict(scores)\n",
    "# Transpose\n",
    "data = data.T\n",
    "\n",
    "# display(data)  # <- good for a truncated print out\n",
    "# https://www.geeksforgeeks.org/how-to-print-an-entire-pandas-dataframe-in-python/\n",
    "# display(data.to_string())  # <- looks weird\n",
    "print(data.to_string())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c3ddc8",
   "metadata": {},
   "source": [
    "Q2 B) Think of three words that identify membership in one of the protected classes (choose only one class): race, color, religion, or national origin. For each row in your selected BATS_3.0 file, compute the similarity between your target word and each of your three words. Indicate when there are noticeable differences in the similarity scores based on membership in the protected class. Provide your results in table format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c403d64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between target word and each of three words from protected class: national origin:\n",
      "               0         1         2\n",
      "0            ant   african  -0.02331\n",
      "1            ant  american -0.014413\n",
      "2            ant   chinese -0.043652\n",
      "3          apple   african -0.108922\n",
      "4          apple  american -0.031242\n",
      "5          apple   chinese -0.001866\n",
      "6     blackboard   african -0.118044\n",
      "7     blackboard  american -0.069308\n",
      "8     blackboard   chinese -0.025206\n",
      "9          blood   african  0.020525\n",
      "10         blood  american -0.065638\n",
      "11         blood   chinese -0.001763\n",
      "12     blueberry   african  0.107284\n",
      "13     blueberry  american -0.084573\n",
      "14     blueberry   chinese  0.023756\n",
      "15      broccoli   african -0.019525\n",
      "16      broccoli  american -0.006587\n",
      "17      broccoli   chinese -0.084253\n",
      "18        bruise   african -0.018624\n",
      "19        bruise  american -0.051657\n",
      "20        bruise   chinese -0.142612\n",
      "21       cabbage   african  0.027477\n",
      "22       cabbage  american -0.066327\n",
      "23       cabbage   chinese  0.029704\n",
      "24        carrot   african  -0.05758\n",
      "25        carrot  american -0.091398\n",
      "26        carrot   chinese -0.043109\n",
      "27   cauliflower   african  0.050677\n",
      "28   cauliflower  american  0.019855\n",
      "29   cauliflower   chinese  0.042827\n",
      "30        celery   african  0.045113\n",
      "31        celery  american -0.059775\n",
      "32        celery   chinese  0.100531\n",
      "33        cherry   african  0.010406\n",
      "34        cherry  american  0.016308\n",
      "35        cherry   chinese -0.041911\n",
      "36     chocolate   african -0.043909\n",
      "37     chocolate  american -0.011377\n",
      "38     chocolate   chinese  0.088184\n",
      "39         cloud   african -0.120791\n",
      "40         cloud  american -0.086336\n",
      "41         cloud   chinese -0.151692\n",
      "42          coal   african  0.043159\n",
      "43          coal  american -0.063821\n",
      "44          coal   chinese -0.102578\n",
      "45        coffee   african  0.095358\n",
      "46        coffee  american  0.020197\n",
      "47        coffee   chinese  0.040352\n",
      "48     cranberry   african  0.078812\n",
      "49     cranberry  american -0.058909\n",
      "50     cranberry   chinese  0.065395\n",
      "51         cream   african   0.03126\n",
      "52         cream  american -0.013076\n",
      "53         cream   chinese   0.06882\n",
      "54          crow   african  0.129586\n",
      "55          crow  american  0.192022\n",
      "56          crow   chinese  0.049833\n",
      "57      cucumber   african -0.022711\n",
      "58      cucumber  american -0.062825\n",
      "59      cucumber   chinese -0.041572\n",
      "60       emerald   african  0.059782\n",
      "61       emerald  american  0.024662\n",
      "62       emerald   chinese -0.026351\n",
      "63          frog   african -0.015949\n",
      "64          frog  american -0.001689\n",
      "65          frog   chinese -0.024228\n",
      "66        grapes   african -0.043244\n",
      "67        grapes  american -0.106721\n",
      "68        grapes   chinese -0.042963\n",
      "69         grass   african  0.021803\n",
      "70         grass  american -0.078633\n",
      "71         grass   chinese  0.004732\n",
      "72        leaves   african -0.015702\n",
      "73        leaves  american  -0.10596\n",
      "74        leaves   chinese -0.015529\n",
      "75          milk   african  -0.00043\n",
      "76          milk  american -0.023517\n",
      "77          milk   chinese  0.092944\n",
      "78         paper   african -0.002318\n",
      "79         paper  american  -0.01711\n",
      "80         paper   chinese  0.004734\n",
      "81       parsley   african -0.042133\n",
      "82       parsley  american -0.058983\n",
      "83       parsley   chinese  0.020827\n",
      "84        pepper   african   0.11233\n",
      "85        pepper  american  0.010069\n",
      "86        pepper   chinese  0.030423\n",
      "87        potato   african  0.065262\n",
      "88        potato  american  0.023001\n",
      "89        potato   chinese  0.045317\n",
      "90        radish   african  0.022052\n",
      "91        radish  american -0.074765\n",
      "92        radish   chinese  0.070388\n",
      "93         raven   african -0.003932\n",
      "94         raven  american  0.066115\n",
      "95         raven   chinese   0.00835\n",
      "96          rose   african  0.182046\n",
      "97          rose  american  0.151537\n",
      "98          rose   chinese -0.037605\n",
      "99          ruby   african -0.001984\n",
      "100         ruby  american  0.109451\n",
      "101         ruby   chinese -0.079896\n",
      "102         salt   african  0.008389\n",
      "103         salt  american  0.005882\n",
      "104         salt   chinese -0.028198\n",
      "105     sapphire   african -0.030863\n",
      "106     sapphire  american  0.041551\n",
      "107     sapphire   chinese -0.023181\n",
      "108          sea   african  0.104766\n",
      "109          sea  american -0.050821\n",
      "110          sea   chinese  -0.01741\n",
      "111          sky   african -0.029269\n",
      "112          sky  american -0.018682\n",
      "113          sky   chinese -0.032061\n",
      "114         snow   african  0.023703\n",
      "115         snow  american -0.005688\n",
      "116         snow   chinese -0.019209\n",
      "117         soil   african  0.044015\n",
      "118         soil  american  0.037046\n",
      "119         soil   chinese -0.072177\n",
      "120      spinach   african  0.003485\n",
      "121      spinach  american -0.068013\n",
      "122      spinach   chinese  0.041408\n",
      "123        sugar   african  0.078022\n",
      "124        sugar  american  0.009119\n",
      "125        sugar   chinese  0.040515\n",
      "126          sun   african -0.045166\n",
      "127          sun  american -0.020564\n",
      "128          sun   chinese  0.115022\n",
      "129         swan   african -0.002829\n",
      "130         swan  american   0.07223\n",
      "131         swan   chinese -0.026455\n",
      "132          tea   african  0.141133\n",
      "133          tea  american  0.094102\n",
      "134          tea   chinese  0.163353\n",
      "135       tomato   african  0.013456\n",
      "136       tomato  american   -0.0725\n",
      "137       tomato   chinese  0.054584\n",
      "138   toothpaste   african  0.028104\n",
      "139   toothpaste  american -0.037669\n",
      "140   toothpaste   chinese   0.01675\n",
      "141      yoghurt   african  0.043475\n",
      "142      yoghurt  american -0.125141\n",
      "143      yoghurt   chinese -0.028097\n",
      "\n",
      "\n",
      "Noticeable differences: "
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>blood</td>\n",
       "      <td>african</td>\n",
       "      <td>0.020525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>blueberry</td>\n",
       "      <td>american</td>\n",
       "      <td>-0.084573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cabbage</td>\n",
       "      <td>american</td>\n",
       "      <td>-0.066327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>celery</td>\n",
       "      <td>american</td>\n",
       "      <td>-0.059775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cherry</td>\n",
       "      <td>chinese</td>\n",
       "      <td>-0.041911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>chocolate</td>\n",
       "      <td>chinese</td>\n",
       "      <td>0.088184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>coal</td>\n",
       "      <td>african</td>\n",
       "      <td>0.043159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>cranberry</td>\n",
       "      <td>american</td>\n",
       "      <td>-0.058909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>cream</td>\n",
       "      <td>american</td>\n",
       "      <td>-0.013076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>emerald</td>\n",
       "      <td>chinese</td>\n",
       "      <td>-0.026351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>grass</td>\n",
       "      <td>american</td>\n",
       "      <td>-0.078633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>milk</td>\n",
       "      <td>chinese</td>\n",
       "      <td>0.092944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>paper</td>\n",
       "      <td>chinese</td>\n",
       "      <td>0.004734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>parsley</td>\n",
       "      <td>chinese</td>\n",
       "      <td>0.020827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>radish</td>\n",
       "      <td>american</td>\n",
       "      <td>-0.074765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>raven</td>\n",
       "      <td>african</td>\n",
       "      <td>-0.003932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>rose</td>\n",
       "      <td>chinese</td>\n",
       "      <td>-0.037605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>ruby</td>\n",
       "      <td>american</td>\n",
       "      <td>0.109451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>salt</td>\n",
       "      <td>chinese</td>\n",
       "      <td>-0.028198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>sapphire</td>\n",
       "      <td>american</td>\n",
       "      <td>0.041551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>sea</td>\n",
       "      <td>african</td>\n",
       "      <td>0.104766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>snow</td>\n",
       "      <td>african</td>\n",
       "      <td>0.023703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>soil</td>\n",
       "      <td>chinese</td>\n",
       "      <td>-0.072177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>spinach</td>\n",
       "      <td>american</td>\n",
       "      <td>-0.068013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>sun</td>\n",
       "      <td>chinese</td>\n",
       "      <td>0.115022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>swan</td>\n",
       "      <td>american</td>\n",
       "      <td>0.072230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>tomato</td>\n",
       "      <td>american</td>\n",
       "      <td>-0.072500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>toothpaste</td>\n",
       "      <td>american</td>\n",
       "      <td>-0.037669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>yoghurt</td>\n",
       "      <td>african</td>\n",
       "      <td>0.043475</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1         2\n",
       "0        blood   african  0.020525\n",
       "1    blueberry  american -0.084573\n",
       "2      cabbage  american -0.066327\n",
       "3       celery  american -0.059775\n",
       "4       cherry   chinese -0.041911\n",
       "5    chocolate   chinese  0.088184\n",
       "6         coal   african  0.043159\n",
       "7    cranberry  american -0.058909\n",
       "8        cream  american -0.013076\n",
       "9      emerald   chinese -0.026351\n",
       "10       grass  american -0.078633\n",
       "11        milk   chinese  0.092944\n",
       "12       paper   chinese  0.004734\n",
       "13     parsley   chinese  0.020827\n",
       "14      radish  american -0.074765\n",
       "15       raven   african -0.003932\n",
       "16        rose   chinese -0.037605\n",
       "17        ruby  american  0.109451\n",
       "18        salt   chinese -0.028198\n",
       "19    sapphire  american  0.041551\n",
       "20         sea   african  0.104766\n",
       "21        snow   african  0.023703\n",
       "22        soil   chinese -0.072177\n",
       "23     spinach  american -0.068013\n",
       "24         sun   chinese  0.115022\n",
       "25        swan  american  0.072230\n",
       "26      tomato  american -0.072500\n",
       "27  toothpaste  american -0.037669\n",
       "28     yoghurt   african  0.043475"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Protected class: national origin\n",
    "protected_class_words = ['african', 'american', 'chinese']\n",
    "\n",
    "scores = {}\n",
    "index = 0\n",
    "for target, _ in data_list:\n",
    "    for word in protected_class_words:\n",
    "        try:\n",
    "            score = (target, word, newmodel.similarity(target, word))\n",
    "            scores[index] = score\n",
    "            index += 1\n",
    "        except KeyError:\n",
    "            # print(f\"{word} not found.\")\n",
    "            continue\n",
    "\n",
    "data = pd.DataFrame.from_dict(scores)\n",
    "\n",
    "data = data.T\n",
    "\n",
    "print('Similarity between target word and each of three words from protected class: national origin:')\n",
    "print(data.to_string())\n",
    "\n",
    "index = 0\n",
    "tuple_list = []\n",
    "noticeable_differences = []\n",
    "for key in scores.keys():\n",
    "    item = scores[key]\n",
    "    tuple_list.append(item)\n",
    "    index += 1\n",
    "    if index % 3 == 0:\n",
    "        value_0 = tuple_list[0][2]\n",
    "        value_1 = tuple_list[1][2]\n",
    "        value_2 = tuple_list[2][2]\n",
    "        # Significant: one is negative and the others are positive, or vice versa\n",
    "        # There are other forms of significance, but they are subtler and harder to distinguish programmatically;\n",
    "        # this should be good enough for now\n",
    "        if value_0 < 0 and value_1 > 0 and value_2 > 0:  # Only 0 -\n",
    "            noticeable_differences.append(tuple_list[0])\n",
    "        elif value_0 > 0 and value_1 < 0 and value_2 > 0:  # Only 1 -\n",
    "            noticeable_differences.append(tuple_list[1])\n",
    "        elif value_0 > 0 and value_1 > 0 and value_2 < 0:  # Only 2 -\n",
    "            noticeable_differences.append(tuple_list[2])\n",
    "        elif value_0 < 0 and value_1 < 0 and value_2 > 0:  # Only 2 +\n",
    "            noticeable_differences.append(tuple_list[2])\n",
    "        elif value_0 < 0 and value_1 > 0 and value_2 < 0:  # Only 1 +\n",
    "            noticeable_differences.append(tuple_list[1])\n",
    "        elif value_0 > 0 and value_1 < 0 and value_2 < 0:  # Only 0 +\n",
    "            noticeable_differences.append(tuple_list[0])\n",
    "        tuple_list = []\n",
    "df_nd = pd.DataFrame(noticeable_differences)\n",
    "print('\\n\\nNoticeable differences: ', end='')\n",
    "display(df_nd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e21aef",
   "metadata": {},
   "source": [
    "Q3: Sentences:\n",
    "\n",
    "a. Complete some sentences with your own word analogies. Use the Word2Vec model to find the similarity measure between your pair of words. Provide your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ac7a3db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "judge, bench -> 0.30267337\n",
      "genius, moron -> 0.12846608\n",
      "jail, warden -> 0.27777424\n",
      "line, polygon -> 0.22809683\n",
      "dutch, netherlands -> 0.41922888\n",
      "king, queen -> 0.5685571\n",
      "liquid, solid -> 0.6546474\n",
      "sad, happy -> 0.44885093\n",
      "teacher, school -> 0.5326567\n",
      "japan, sushi -> 0.01186634\n",
      "dog, shed -> 0.10359062\n",
      "sky, blue -> 0.4439698\n",
      "computer, floppy -> 0.32768583\n",
      "house, atom -> 0.07146792\n",
      "sickness, health -> 0.19527604\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "1- king is to throne as judge is to ___? bench\n",
    "2- giant is to dwarf as genius is to ___? moron\n",
    "3- college is to dean as jail is to ___? warden\n",
    "4- arc is to circle as line is to ___? polygon\n",
    "5- French is to France as Dutch is to ___? Netherlands\n",
    "6- man is to woman as king is to ___? queen\n",
    "7- water is to ice as liquid is to ___? solid\n",
    "8- bad is to good as sad is to ___? happy\n",
    "9- nurse is to hospital as teacher is to ___? school\n",
    "10- usa is to pizza as japan is to ___? sushi\n",
    "11- human is to house as dog is to ___? shed\n",
    "12- grass is to green as sky is to ___? blue\n",
    "13- video is to cassette as computer is to ____? floppy\n",
    "14- universe is to planet as house is to ____? atom\n",
    "15- poverty is to wealth as sickness is to ___? health\n",
    "\"\"\"\n",
    "\n",
    "all_pairs = []\n",
    "all_pairs.append(['king','throne','judge','bench'])\n",
    "all_pairs.append(['giant','dwarf','genius','moron'])\n",
    "all_pairs.append(['college','dean','jail','warden'])\n",
    "all_pairs.append(['arc','circle','line','polygon'])\n",
    "all_pairs.append(['french','france','dutch','netherlands'])\n",
    "all_pairs.append(['man','woman','king','queen'])\n",
    "all_pairs.append(['water','ice','liquid','solid'])\n",
    "all_pairs.append(['bad','good','sad','happy'])\n",
    "all_pairs.append(['nurse','hospital','teacher','school'])\n",
    "all_pairs.append(['usa','pizza','japan','sushi'])\n",
    "all_pairs.append(['human','house','dog','shed'])\n",
    "all_pairs.append(['grass','green','sky','blue'])\n",
    "all_pairs.append(['video','cassette','computer','floppy'])\n",
    "all_pairs.append(['universe','planet','house','atom'])\n",
    "all_pairs.append(['poverty','wealth','sickness','health'])\n",
    "my_scores = []\n",
    "for tup in all_pairs:\n",
    "    score = newmodel.similarity(tup[2], tup[3])\n",
    "    print(tup[2] + ', ' + tup[3] + ' -> ' + str(score))\n",
    "    my_scores.append(score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ee970d",
   "metadata": {},
   "source": [
    "Q3 b. Use the Word2Vec model to find the word analogy and corresponding similarity score. Provide your results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f657e55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "king is to throne as judge is to : [('prosecution', 0.5186458230018616)]\n",
      "giant is to dwarf as genius is to : [('theorist', 0.4280889630317688)]\n",
      "college is to dean as jail is to : [('peress', 0.5444425940513611)]\n",
      "arc is to circle as line is to : [('lines', 0.4287526607513428)]\n",
      "french is to france as dutch is to : [('netherlands', 0.6044681072235107)]\n",
      "man is to woman as king is to : [('queen', 0.5532454252243042)]\n",
      "water is to ice as liquid is to : [('solid', 0.4500039219856262)]\n",
      "bad is to good as sad is to : [('glory', 0.440381795167923)]\n",
      "nurse is to hospital as teacher is to : [('institution', 0.48289817571640015)]\n",
      "usa is to pizza as japan is to : [('dishes', 0.5763506293296814)]\n",
      "human is to house as dog is to : [('hound', 0.4231664538383484)]\n",
      "grass is to green as sky is to : [('blue', 0.5478643178939819)]\n",
      "video is to cassette as computer is to : [('peripherals', 0.6654507517814636)]\n",
      "universe is to planet as house is to : [('houses', 0.4264702796936035)]\n",
      "poverty is to wealth as sickness is to : [('impious', 0.49606096744537354)]\n"
     ]
    }
   ],
   "source": [
    "Word2Vec_scores = []\n",
    "for tup in all_pairs:\n",
    "    print(f\"{tup[0]} is to {tup[1]} as {tup[2]} is to : \", end='')\n",
    "    result = newmodel.most_similar(positive=[tup[2], tup[1]], negative=[tup[0]], topn=1)\n",
    "    print(result)\n",
    "    score = result[0][1]\n",
    "    Word2Vec_scores.append(score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3572db4d",
   "metadata": {},
   "source": [
    "Q3 c Lastly, compute and print the correlation between the vector of similarity scores from your analogies versus the Word2Vec analogy-generated similarity scores. What is the strength of the correlation?\n",
    "\n",
    "* .00-.19 “very weak” correlation \n",
    "* .20-.39 “weak” correlation\n",
    "* .40-.59 “moderate” correlation \n",
    "* .60-.79 “strong” correlation\n",
    "* .80-1.0 “very strong” correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f392d06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my score: 0.302673 -> Word2Vec's score: 0.518646\n",
      "my score: 0.128466 -> Word2Vec's score: 0.428089\n",
      "my score: 0.277774 -> Word2Vec's score: 0.544443\n",
      "my score: 0.228097 -> Word2Vec's score: 0.428753\n",
      "my score: 0.419229 -> Word2Vec's score: 0.604468\n",
      "my score: 0.568557 -> Word2Vec's score: 0.553245\n",
      "my score: 0.654647 -> Word2Vec's score: 0.450004\n",
      "my score: 0.448851 -> Word2Vec's score: 0.440382\n",
      "my score: 0.532657 -> Word2Vec's score: 0.482898\n",
      "my score: 0.011866 -> Word2Vec's score: 0.576351\n",
      "my score: 0.103591 -> Word2Vec's score: 0.423166\n",
      "my score: 0.443970 -> Word2Vec's score: 0.547864\n",
      "my score: 0.327686 -> Word2Vec's score: 0.665451\n",
      "my score: 0.071468 -> Word2Vec's score: 0.426470\n",
      "my score: 0.195276 -> Word2Vec's score: 0.496061\n",
      "\n",
      "Correlation coefficient: 0.16294155504979496\n",
      "\n",
      "Correlation category: Very weak correlation\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for index in range(len(all_pairs)):\n",
    "    print(f\"my score: {my_scores[index]:3f} -> Word2Vec's score: {Word2Vec_scores[index]:3f}\")\n",
    "    index += 1\n",
    "\n",
    "# https://numpy.org/doc/stable/reference/generated/numpy.corrcoef.html\n",
    "print('\\nCorrelation coefficient: ', end='')\n",
    "print(np.corrcoef(my_scores, Word2Vec_scores)[0][1])\n",
    "\n",
    "# Copied from my A3 - AI ML 1:\n",
    "def correlation_category(correlation_value):\n",
    "    output = ''\n",
    "    if abs(correlation_value) < 0.2:\n",
    "        output += 'Very weak correlation'\n",
    "    elif abs(correlation_value) < 0.4:\n",
    "        output += 'Weak correlation'\n",
    "    elif abs(correlation_value) < 0.6:\n",
    "        output += 'Moderate correlation'\n",
    "    elif abs(correlation_value) < 0.8:\n",
    "        output += 'Strong correlation'\n",
    "    else:\n",
    "        output += 'Very strong correlation'\n",
    "    return output\n",
    "\n",
    "print('\\nCorrelation category: ' + correlation_category(np.corrcoef(my_scores, Word2Vec_scores)[0][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38fdb043",
   "metadata": {},
   "source": [
    "Task Set #2\n",
    "\n",
    "Q1: Each image in the dataset has a unique value representing age, gender, and race based on the following legend:\n",
    "\n",
    "* age: indicates the age of the person in the picture and can range from 0 to 116.\n",
    "* gender: indicates the gender of the person and is either 0 (male) or 1 (female).\n",
    "* race: indicates the race of the person and can from 0 to 4, denoting White, Black, Asian, Indian, and Others (like Hispanic, Latino, Middle Eastern).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b405aac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "files count: 9781\n",
      "Looking for duplicates & bad file names. \".\" = 1 file checked. \"#\" = duplicate found...\n",
      ".........................#...............#..........#..#..#.#.#..#.........................#.....#............\n",
      ".........#.........#...##......#...............................#.....#..#....................#...............\n",
      ".#...........................................................#.........#.......#........#..#..............\n",
      "................................................................................#....................\n",
      "........#...............................................................#.............................\n",
      "...............................#.....#...............................................#....#.#.....#.....#..\n",
      ".........#............................................................#..............................#.\n",
      "..................................#..........................#.......#.................................\n",
      "......................................................#.....#.........................................\n",
      "....................................................#................................................\n",
      "............#..........#..............................................................................\n",
      "......................#...........#.........................................................#..........\n",
      "....#..............##...#................................................##...........#....................\n",
      "...............................#...............................#.........#.............................\n",
      "........#...........................................#.................#................................\n",
      "...............................................................#...............#......................\n",
      ".....................#...........##.....................#..#........................##.........#............\n",
      "..#..................................................##..................#..............................\n",
      ".#.............#..............................#.#................#...........................#............\n",
      "......#....................................................##...#...#....................................\n",
      "............#........................................................................................\n",
      "..................................#........#......#....................................................\n",
      "..................................#..........................#.#.......................................\n",
      ".......................................................###.......#....#...#.#.............................#.\n",
      ".#.#.#..#.#.#.##.#....#..........................................#.............................................\n",
      ".......................................#.............................................................\n",
      "..................#..............#........................#............................................\n",
      "............#........................................................................#.............#...\n",
      "....................................................................................................\n",
      "..#.....................................................................................#.............\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      ".............................................................#.......................................\n",
      "....................................................................................................\n",
      ".........................................................................#...........................\n",
      "....................................................................................................\n",
      "...................................................#............................................#.....\n",
      "....................................................................................................\n",
      ".................................#.....................#.......#.......................................\n",
      ".........................................#...........................................................\n",
      "...............................................#..............#.........................#..............\n",
      "....#............................................................#....................................\n",
      "....................................#................................................................\n",
      "...................#........................................#.........................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................#................................................................\n",
      "....................................................................................................\n",
      "...........................................#.........................................................\n",
      "........................#............................................................................\n",
      "..............................................................#......................................\n",
      ".................................................................................#...................\n",
      "....................................................................................................\n",
      ".....................................#.........................#..........................#.#.........#..\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      ".........................................................#...........................................\n",
      "....................................................................................................\n",
      "............................................................................#............#............\n",
      "..............................................................................................#......\n",
      "...........................................................#.........................................\n",
      ".........................................#..#.........................................................\n",
      "....................................................................................................\n",
      ".....###...#............................................................................................\n",
      "..................................................................................................##..\n",
      ".......................#.............................#.#...............................................\n",
      ".....................................................................#...............................\n",
      ".............................................................#.......#................................\n",
      "..........................#..........................................................................\n",
      "...........................................................#.........................................\n",
      "...............#..............................................................#.................#......\n",
      "............................................................................................#........\n",
      ".....#...............................................#............................................#....\n",
      "..................#..#............#.......................#.............................................\n",
      ".................................\n",
      "bad file name: 61_1_20170109142408075.jpg.chip.jpg\n",
      "..#..#..\n",
      "bad file name: 61_3_20170109150557335.jpg.chip.jpg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".................................................##.#...#........\n",
      "..#..................................................................................................\n",
      "........................................................#....................#........................\n",
      ".......................#............#...........................................#......................\n",
      "............................................#..##......................................................\n",
      "............................................#.....#...................................................\n",
      "#....................................#...............................##...............#..................\n",
      "..........#....#..............................................#........................................\n",
      "..................#................................#.................#........#.........................\n",
      "....................................................................................................\n",
      "............#....................#................................................#....................\n",
      "..#..........................................................#.............#...........................\n",
      "......##..............................................................................................\n",
      "....................................................................................................\n",
      ".............................#.............#.....................................#...............#......\n",
      "....................................................................................................\n",
      "....................................................................#................................\n",
      ".....#.........................................#......................................................\n",
      "..................................................................\n",
      "Done in 3 hour(s), 17 minute(s), and 31 second(s)\n",
      "Found 224 duplicates.\n"
     ]
    }
   ],
   "source": [
    "# https://pyimagesearch.com/2014/09/15/python-compare-two-images/\n",
    "def mse(vals_1, vals_2):\n",
    "    # the 'Mean Squared Error' between the two images is the\n",
    "    # sum of the squared difference between the two images;\n",
    "    # NOTE: the two images must have the same dimension\n",
    "    err = np.sum((vals_1[0] - vals_2[0]) ** 2)\n",
    "    err /= float(vals_1[1] * vals_1[2])\n",
    "\n",
    "    # return the MSE, the lower the error, the more \"similar\"\n",
    "    # the two images are\n",
    "    return err\n",
    "\n",
    "\n",
    "def get_images_values(files, folder_path):\n",
    "    image_values = {}\n",
    "\n",
    "    for image_file in files:\n",
    "        image = cv2.imread(folder_path + image_file)\n",
    "        if image is not None:\n",
    "            image_values[image_file] = [image.astype('float'), image.shape[0], image.shape[1]]\n",
    "    return image_values\n",
    "\n",
    "\n",
    "def get_time(seconds):\n",
    "    # Copied from my CS 7641 Assignment 2\n",
    "    if int(seconds / 60) == 0:\n",
    "        if int(seconds) == 0:\n",
    "            return str(round(seconds, 3)) + ' seconds'\n",
    "        return str(int(seconds)) + ' second(s)'\n",
    "    minutes = int(seconds / 60)\n",
    "    seconds = int(seconds % 60)\n",
    "    if int(minutes / 60) == 0:\n",
    "        return str(minutes) + ' minute(s) and ' + str(seconds) + ' second(s)'\n",
    "    hours = int(minutes / 60)\n",
    "    minutes = int(minutes % 60)\n",
    "    # Assuming this won't be called for any time span greater than 24 hours\n",
    "    return str(hours) + ' hour(s), ' + str(minutes) + ' minute(s), and ' + str(seconds) + ' second(s)'\n",
    "\n",
    "\n",
    "path = 'crop_part1/'\n",
    "dir_list = os.listdir(path)\n",
    "dir_list.sort()\n",
    "compare_list = os.listdir(path)\n",
    "compare_list.sort()\n",
    "images_values = get_images_values(dir_list, path)\n",
    "\n",
    "print('files count: ' + str(len(dir_list)))\n",
    "\"\"\"\n",
    "# fileA = '96_1_0_20170110183855839.jpg.chip.jpg'  # same\n",
    "# fileB = '96_1_1_20170110183853718.jpg.chip.jpg'  # same\n",
    "# fileC = '96_1_0_20170110182515404.jpg.chip.jpg'  # different\n",
    "# m = mse(images_values[fileA], images_values[fileB])\n",
    "# print('mse (A & B): ' + str(m))  # 0.0\n",
    "# m = mse(images_values[fileA], images_values[fileC])\n",
    "# print('mse (A & C): ' + str(m))  # 7251.330525\n",
    "\n",
    "# fileA = '90_1_0_20170110182841384.jpg.chip.jpg'  # same, but blurry :/, mse > 100\n",
    "# fileB = '96_1_0_20170110182019881.jpg.chip.jpg'\n",
    "# m = mse(fileA, fileB, path)\n",
    "# print('mse (A & B): ' + str(m)) # MSE = 187.775225\n",
    "# Taking this as a baseline for nearly identical images\n",
    "\"\"\"\n",
    "\n",
    "duplicates = []\n",
    "anomalies = []\n",
    "index = 0\n",
    "print('Looking for duplicates & bad file names. \".\" = 1 file checked. \"#\" = duplicate found...')\n",
    "start = time.time()\n",
    "for file in dir_list:\n",
    "    if file not in images_values.keys():\n",
    "        # File didn't make it into images_values (like .DS_Store) - skip it\n",
    "        compare_list.remove(file)\n",
    "        anomalies.append(file)\n",
    "        continue\n",
    "    file_parts = file.split('_')\n",
    "    if len(file_parts) < 4:\n",
    "        print('\\nbad file name: ' + file)\n",
    "    if file not in compare_list:\n",
    "        # Previously detected duplicate\n",
    "        continue\n",
    "    compare_list.remove(file)\n",
    "    for duplicate in duplicates:\n",
    "        if duplicate in compare_list:\n",
    "            compare_list.remove(duplicate)\n",
    "    for file_other in compare_list:\n",
    "        m = mse(images_values[file], images_values[file_other])\n",
    "        if m < 188:\n",
    "            # print(file + ' seems to be a duplicate of ' + file_other)\n",
    "            print('#', end='')\n",
    "            # duplicates.append(file)\n",
    "            duplicates.append(file_other)\n",
    "            \"\"\"\n",
    "            12 duplicates found below 0.35% (unsorted), including:\n",
    "            21_0_4_20161223214826657.jpg.chip.jpg, 6_1_4_20170103230723185.jpg.chip.jpg\n",
    "            I highly doubt subject is really 21 years old in this picture\n",
    "            \"\"\"\n",
    "    print('.', end='')\n",
    "    index += 1\n",
    "    if index % 100 == 0:\n",
    "        print()\n",
    "\n",
    "end = time.time()\n",
    "print('\\nDone in ' + get_time(end - start))\n",
    "print('Found ' + str(len(duplicates)) + ' duplicates.')\n",
    "for duplicate in duplicates:\n",
    "    if duplicate in dir_list:\n",
    "        dir_list.remove(duplicate)\n",
    "\n",
    "for anomaly in anomalies:\n",
    "    if anomaly in dir_list:\n",
    "        dir_list.remove(anomaly)\n",
    "\n",
    "# Data has been cleaned up some. (Could probably do more, but this should be good enough for now)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60160199",
   "metadata": {},
   "source": [
    "Compute and document the frequency of images associated with each subgroup for age (subdivide based on - (0-20), (21,40), (41,60), (61,80), (81, 116)), gender (0,1), and race (0 to 4). Which subgroup in each age, gender, and race category has the largest representation? Which subgroup in each age, gender, and race category has the least representation? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2de4e29f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Age group with largest representation: (0, 20] (44.45%)\n",
      "Age group with least representation: (81, 116] (3.54%)\n",
      "Gender group with largest representation: female (55.07%)\n",
      "Gender group with least representation: male (44.93%)\n",
      "Race group with largest representation: white (53.83%)\n",
      "Race group with least representation: black (4.16%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sum up counts for each group...\n",
    "# age_0_20 = 0\n",
    "# age_21_40 = 0\n",
    "# age_41_60 = 0\n",
    "# age_61_80 = 0\n",
    "# age_81_116 = 0\n",
    "# male = 0\n",
    "# female = 0\n",
    "# white = 0\n",
    "# black = 0\n",
    "# asian = 0\n",
    "# indian = 0\n",
    "# others = 0\n",
    "\n",
    "# for file in dir_list:\n",
    "#     file_parts = file.split('_')\n",
    "#     age = int(file_parts[0])\n",
    "#     if age < 21:\n",
    "#         age_0_20 += 1\n",
    "#     elif age < 41:\n",
    "#         age_21_40 += 1\n",
    "#     elif age < 61:\n",
    "#         age_41_60 += 1\n",
    "#     elif age < 81:\n",
    "#         age_61_80 += 1\n",
    "#     else:\n",
    "#         age_81_116 += 1\n",
    "#\n",
    "#     if len(file_parts) < 4:\n",
    "#         continue\n",
    "#\n",
    "#     gender = int(file_parts[1])\n",
    "#     if gender == 0:\n",
    "#         male += 1\n",
    "#     else:\n",
    "#         female += 1\n",
    "#\n",
    "#     race = int(file_parts[2])\n",
    "#     if race == 0:\n",
    "#         white += 1\n",
    "#     elif race == 1:\n",
    "#         black += 1\n",
    "#     elif race == 2:\n",
    "#         asian += 1\n",
    "#     elif race == 3:\n",
    "#         indian += 1\n",
    "#     else:\n",
    "#         others += 1\n",
    "\n",
    "# On second thought, this is getting too tedious. Let's try to do this a little more smartly...\n",
    "genders = {0: 'male', 1: 'female'}\n",
    "races = {0: 'white', 1: 'black', 2: 'asian', 3: 'indian', 4: 'other'}\n",
    "img_names = []\n",
    "for file in dir_list:\n",
    "    img_names.append(file.replace('.jpg.chip.jpg', ''))\n",
    "\n",
    "age_pattern = re.compile(r\"(^\\d{1,3}).+\")\n",
    "gender_pattern = re.compile(r\"^\\d{1,3}_(\\d).+\")\n",
    "race_pattern = re.compile(r\"^\\d{1,3}_\\d_(\\d).+\")\n",
    "age_list = []\n",
    "gender_list = []\n",
    "race_list = []\n",
    "for name in img_names:\n",
    "    age_list.append(int(re.match(age_pattern, name).group(1)))\n",
    "    gender_list.append(int(re.match(gender_pattern, name).group(1)))\n",
    "    race_list.append(int(re.match(race_pattern, name).group(1)))\n",
    "\n",
    "img_df = pd.DataFrame({\n",
    "    \"img_name\": img_names, \"ages\": age_list,\n",
    "    \"genders\": gender_list, \"races\": race_list\n",
    "    })\n",
    "\n",
    "bins = pd.IntervalIndex.from_tuples([(0, 20), (21, 40), (41, 60), (61, 80), (81, 116)])\n",
    "img_df[\"age\"] = pd.cut(img_df[\"ages\"], bins)\n",
    "\n",
    "img_df[\"gender\"] = img_df[\"genders\"].map(genders)\n",
    "\n",
    "img_df[\"race\"] = img_df[\"races\"].map(races)\n",
    "\n",
    "df = img_df[[\"age\", \"gender\", \"race\"]]\n",
    "\n",
    "age_value_counts = df[\"age\"].value_counts()\n",
    "age_group_max = age_value_counts[age_value_counts == age_value_counts.max()]\n",
    "age_proportion_max = 100 * (age_group_max.values[0] / age_value_counts.sum())\n",
    "age_group_min = age_value_counts[age_value_counts == age_value_counts.min()]\n",
    "age_proportion_min = 100 * (age_group_min.values[0] / age_value_counts.sum())\n",
    "print(f\"Age group with largest representation: {age_group_max.index[0]} ({age_proportion_max:.2f}%)\")\n",
    "print(f\"Age group with least representation: {age_group_min.index[0]} ({age_proportion_min:.2f}%)\")\n",
    "\n",
    "gender_value_counts = df[\"gender\"].value_counts()\n",
    "gender_group_max = gender_value_counts[gender_value_counts == gender_value_counts.max()]\n",
    "gender_proportion_max = 100 * (gender_group_max.values[0] / gender_value_counts.sum())\n",
    "gender_group_min = gender_value_counts[gender_value_counts == gender_value_counts.min()]\n",
    "gender_proportion_min = 100 * (gender_group_min.values[0] / gender_value_counts.sum())\n",
    "print(f\"Gender group with largest representation: {gender_group_max.index[0]} ({gender_proportion_max:.2f}%)\")\n",
    "print(f\"Gender group with least representation: {gender_group_min.index[0]} ({gender_proportion_min:.2f}%)\")\n",
    "\n",
    "race_value_counts = df[\"race\"].value_counts()\n",
    "race_group_max = race_value_counts[race_value_counts == race_value_counts.max()]\n",
    "race_proportion_max = 100 * (race_group_max.values[0] / race_value_counts.sum())\n",
    "race_group_min = race_value_counts[race_value_counts == race_value_counts.min()]\n",
    "race_proportion_min = 100 * (race_group_min.values[0] / race_value_counts.sum())\n",
    "print(f\"Race group with largest representation: {race_group_max.index[0]} ({race_proportion_max:.2f}%)\")\n",
    "print(f\"Race group with least representation: {race_group_min.index[0]} ({race_proportion_min:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a750c477",
   "metadata": {},
   "source": [
    "Recreate a table of the age group, gender, and race distributions of subjects based on the UTK dataset subgroups (inspired by the one discussed in the lecture and reposted below). Based on what you’ve learned so far, if an algorithm is trained based on this dataset, which group(s) will be impacted the most? Explain why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dd88db8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>(0, 20]</th>\n",
       "      <th>(21, 40]</th>\n",
       "      <th>(41, 60]</th>\n",
       "      <th>(61, 80]</th>\n",
       "      <th>(81, 116]</th>\n",
       "      <th>total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>female</th>\n",
       "      <td>2250</td>\n",
       "      <td>1518</td>\n",
       "      <td>711</td>\n",
       "      <td>423</td>\n",
       "      <td>223</td>\n",
       "      <td>5125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>male</th>\n",
       "      <td>1889</td>\n",
       "      <td>847</td>\n",
       "      <td>881</td>\n",
       "      <td>462</td>\n",
       "      <td>107</td>\n",
       "      <td>4186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>asian</th>\n",
       "      <td>1003</td>\n",
       "      <td>330</td>\n",
       "      <td>80</td>\n",
       "      <td>43</td>\n",
       "      <td>51</td>\n",
       "      <td>1507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>black</th>\n",
       "      <td>157</td>\n",
       "      <td>96</td>\n",
       "      <td>75</td>\n",
       "      <td>52</td>\n",
       "      <td>14</td>\n",
       "      <td>394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>indian</th>\n",
       "      <td>588</td>\n",
       "      <td>558</td>\n",
       "      <td>150</td>\n",
       "      <td>50</td>\n",
       "      <td>22</td>\n",
       "      <td>1368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>other</th>\n",
       "      <td>524</td>\n",
       "      <td>380</td>\n",
       "      <td>81</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>white</th>\n",
       "      <td>1867</td>\n",
       "      <td>1001</td>\n",
       "      <td>1206</td>\n",
       "      <td>731</td>\n",
       "      <td>241</td>\n",
       "      <td>5046</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        (0, 20]  (21, 40]  (41, 60]  (61, 80]  (81, 116]  total\n",
       "female     2250      1518       711       423        223   5125\n",
       "male       1889       847       881       462        107   4186\n",
       "asian      1003       330        80        43         51   1507\n",
       "black       157        96        75        52         14    394\n",
       "indian      588       558       150        50         22   1368\n",
       "other       524       380        81         9          2    996\n",
       "white      1867      1001      1206       731        241   5046"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on what I've learned so far, if an algorithm is trained based on this dataset, the groups impacted the most \n",
      "will be the minorities - ages 81-116 and individuals whose race is black - because any lessons learned from an \n",
      "algorithm trained on this dataset will be applied to those minorities disproportionately. (The dispartiy between \n",
      "males and females is not so significant to be a concern for being disproportinonately impacted by an algorithm \n",
      "trained based on this dataset.) Moreoever, an exception to this might be any individuals who have been \n",
      "miscategorized (especially if they were incorrectly placed into one the minority categories); for example, \n",
      "21_0_4_20161223214826657.jpg.chip.jpg who is clearly not 21 years old at the time this photo was taken. Finding \n",
      "duplicates may be fairly easy using a mean square error detection on each face, but finding miscategorizations \n",
      "programmatically would be significantly trickier.\n"
     ]
    }
   ],
   "source": [
    "age_gender_table = df[[\"age\", \"gender\"]].value_counts().reset_index(drop=False).rename({0: \"n\"}, axis=1)\n",
    "age_gender_table = age_gender_table.pivot(index=\"gender\", columns=\"age\", values=\"n\").rename_axis(None)\n",
    "age_gender_table.columns.name = None\n",
    "\n",
    "age_race_table = df[[\"age\", \"race\"]].value_counts().reset_index(drop=False).rename({0: \"n\"}, axis=1)\n",
    "age_race_table = age_race_table.pivot(index=\"race\", columns=\"age\", values=\"n\").rename_axis(None)\n",
    "age_race_table.columns.name = None\n",
    "\n",
    "table = pd.concat([age_gender_table, age_race_table])\n",
    "table[\"total\"] = table.sum(axis=1)\n",
    "\n",
    "display(table)\n",
    "\n",
    "print('Based on what I\\'ve learned so far, if an algorithm is trained based on this dataset, the groups impacted the most \\n' + \n",
    "      'will be the minorities - ages 81-116 and individuals whose race is black - because any lessons learned from an \\n' +\n",
    "      'algorithm trained on this dataset will be applied to those minorities disproportionately. (The dispartiy between \\n' +\n",
    "      'males and females is not so significant to be a concern for being disproportinonately impacted by an algorithm \\n' +\n",
    "      'trained based on this dataset.) Moreoever, an exception to this might be any individuals who have been \\n' +\n",
    "      'miscategorized (especially if they were incorrectly placed into one the minority categories); for example, \\n' +\n",
    "      '21_0_4_20161223214826657.jpg.chip.jpg who is clearly not 21 years old at the time this photo was taken. Finding \\n' +\n",
    "      'duplicates may be fairly easy using a mean square error detection on each face, but finding miscategorizations \\n' +\n",
    "      'programmatically would be significantly trickier.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e7f708",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
